{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.regularizers import l2, activity_l2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss, auc, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adagrad,SGD,Adadelta\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import containers\n",
    "from keras.layers.core import Dense, AutoEncoder\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1778)  # for reproducibility\n",
    "need_normalise=True\n",
    "need_validataion=True\n",
    "need_categorical=False\n",
    "save_categorical_file=False\n",
    "nb_epoch=1#400\n",
    "golden_feature=[(\"CoverageField1B\",\"PropertyField21B\"),\n",
    "                (\"GeographicField6A\",\"GeographicField8A\"),\n",
    "                (\"GeographicField6A\",\"GeographicField13A\"),\n",
    "                (\"GeographicField8A\",\"GeographicField13A\"),\n",
    "                (\"GeographicField11A\",\"GeographicField13A\"),\n",
    "                (\"GeographicField8A\",\"GeographicField11A\")]\n",
    "\n",
    "def save2model(submission,file_name,y_pre):\n",
    "    assert len(y_pre)==len(submission)\n",
    "    submission['QuoteConversion_Flag']=y_pre\n",
    "    submission.to_csv(file_name,index=False)\n",
    "    print (\"saved files %s\" % file_name)\n",
    "    \n",
    "from keras.regularizers import l2, activity_l2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.utils import np_utils, generic_utils\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import log_loss, auc, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import Adagrad,SGD,Adadelta\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import containers\n",
    "from keras.layers.core import Dense, AutoEncoder\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "np.random.seed(1778)  # for reproducibility\n",
    "need_normalise=True\n",
    "need_validataion=True\n",
    "need_categorical=False\n",
    "save_categorical_file=False\n",
    "nb_epoch=1#400\n",
    "golden_feature=[(\"CoverageField1B\",\"PropertyField21B\"),\n",
    "                (\"GeographicField6A\",\"GeographicField8A\"),\n",
    "                (\"GeographicField6A\",\"GeographicField13A\"),\n",
    "                (\"GeographicField8A\",\"GeographicField13A\"),\n",
    "                (\"GeographicField11A\",\"GeographicField13A\"),\n",
    "                (\"GeographicField8A\",\"GeographicField11A\")]\n",
    "\n",
    "def save2model(submission,file_name,y_pre):\n",
    "    assert len(y_pre)==len(submission)\n",
    "    submission['QuoteConversion_Flag']=y_pre\n",
    "    submission.to_csv(file_name,index=False)\n",
    "    print (\"saved files %s\" % file_name)\n",
    "\n",
    "def getDummy(df,col):\n",
    "    category_values=df[col].unique()\n",
    "    data=[[0 for i in range(len(category_values))] for i in range(len(df))]\n",
    "    print data\n",
    "    dic_category=dict()\n",
    "    for i,val in enumerate(list(category_values)):\n",
    "        dic_category[str(val)]=i\n",
    "   # print dic_category\n",
    "    for i in range(len(df)):\n",
    "        data[i][dic_category[str(df[col][i])]]=1\n",
    "\n",
    "    data=np.array(data)\n",
    "    for i,val in enumerate(list(category_values)):\n",
    "        df.loc[:,\"_\".join([col,str(val)])]=data[:,i]\n",
    "\n",
    "    return df\n",
    "\n",
    "def generateFileName(model,params):\n",
    "     file_name=\"_\".join([(key+\"_\"+ str(val))for key,val in params.items()])\n",
    "     return model+\"_\"+file_name+\".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train=pd.read_csv('data/train.csv')\n",
    "    test=pd.read_csv('data/test.csv')\n",
    "    train = train.drop(['QuoteNumber','PropertyField6', 'GeographicField10A'], axis=1)\n",
    "    \n",
    "    submission=pd.DataFrame()\n",
    "    submission[\"QuoteNumber\"]= test[\"QuoteNumber\"]\n",
    "\n",
    "    train_y=train['QuoteConversion_Flag'].values\n",
    "    train=train.drop('QuoteConversion_Flag',axis=1)\n",
    "    \n",
    "    test = test.drop(['QuoteNumber','PropertyField6', 'GeographicField10A'],axis=1)\n",
    "    train['Date'] = pd.to_datetime(pd.Series(train['Original_Quote_Date']))\n",
    "    train = train.drop('Original_Quote_Date', axis=1)\n",
    "    train['Year'] = train['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "    train['Month'] = train['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "    train['weekday'] = [train['Date'][i].dayofweek for i in range(len(train['Date']))]\n",
    "    \n",
    "    test['Date'] = pd.to_datetime(pd.Series(test['Original_Quote_Date']))\n",
    "    test = test.drop('Original_Quote_Date', axis=1)\n",
    "    test['Year'] = test['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "    test['Month'] = test['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "    test['weekday'] = [test['Date'][i].dayofweek for i in range(len(test['Date']))]\n",
    "    \n",
    "    train = train.drop('Date', axis=1)\n",
    "    test = test.drop('Date', axis=1)\n",
    "    \n",
    "    #fill na, have no effect on need_categorical method\n",
    "    train = train.fillna(-1)\n",
    "    test = test.fillna(-1)\n",
    "    \n",
    "    for f in test.columns:#\n",
    "        if train[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train[f])+list(test[f]))\n",
    "            train[f] = lbl.transform(list(train[f].values))\n",
    "            test[f] = lbl.transform(list(test[f].values))\n",
    "\n",
    "    #try to encode all params less than 100 to be category\n",
    "    # one column for each caterogy\n",
    "    if need_categorical:\n",
    "        #row bind train and test\n",
    "        x=train.append(test,ignore_index=True)\n",
    "        del train\n",
    "        del test\n",
    "        for f in x.columns:#\n",
    "            category_values= set(list(x[f].unique()))\n",
    "            if len(category_values)<4:\n",
    "                print (f)\n",
    "                x=getDummy(x,f)\n",
    "                #x.drop(f,axis=1)\n",
    "                #all_data.drop(f,axis=1)\n",
    "        test = x.iloc[260753:,]\n",
    "        train = x.iloc[:260753:,]\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    train_y = encoder.fit_transform(train_y).astype(np.int32)\n",
    "    train_y = np_utils.to_categorical(train_y)\n",
    "    #for featureA,featureB in golden_feature:\n",
    "    #    train.loc[:,\"_\".join([featureA,featureB,\"diff\"])]=train[featureA]-train[featureB]\n",
    "    #    test.loc[:,\"_\".join([featureA,featureB,\"diff\"])]=test[featureA]-test[featureB]        \n",
    "\n",
    "    print (\"processsing finished\")\n",
    "    valid=None\n",
    "    valid_y=None\n",
    "    train = np.array(train)\n",
    "    train = train.astype(np.float32)\n",
    "    test=np.array(test)\n",
    "    test=test.astype(np.float32)\n",
    "    if need_normalise:\n",
    "        scaler = StandardScaler().fit(train)\n",
    "        train = scaler.transform(train)\n",
    "        test = scaler.transform(test)\n",
    "    \n",
    "    if need_validataion:\n",
    "        train,valid,train_y,valid_y=train_test_split(train,train_y,test_size=20000,random_state=218)\n",
    "    return [(train,train_y),(test,submission),(valid,valid_y)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "datasets=load_data()\n",
    "X_train, y_train = datasets[0]\n",
    "X_test, submission = datasets[1]\n",
    "X_valid, y_valid = datasets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 'classes')\n",
      "(297, 'dims')\n"
     ]
    }
   ],
   "source": [
    "nb_classes = y_train.shape[1]\n",
    "print(nb_classes, 'classes')\n",
    "\n",
    "dims = X_train.shape[1]\n",
    "print(dims, 'dims')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, input_shape=(dims,)))\n",
    "model.add(Dropout(0.1))#    input dropout\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(360))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(420))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "#opt=SGD(momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "('best_score is:', -1)\n",
      "Epoch 1/1\n",
      "240753/240753 [==============================] - 112s - loss: 0.3029   \n",
      "20000/20000 [==============================] - 1s     \n",
      "(0, 0.95203032934079812)\n",
      "151168/173836 [=========================>....] - ETA: 1s"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8cfbb072be4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#print('Generating submission...')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0my_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;31m#print roc_auc_score(y_test,y_pre)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0msave2model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'keras_nn_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hongjian/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X, batch_size, verbose)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mprobability\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m         '''\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Network returning invalid probability values.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hongjian/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, batch_size, verbose)\u001b[0m\n\u001b[1;32m    714\u001b[0m         '''\n\u001b[1;32m    715\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize_X\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hongjian/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hongjian/anaconda/lib/python2.7/site-packages/keras/utils/generic_utils.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mprev_total_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\b\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprev_total_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/hongjian/anaconda/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file"
     ]
    }
   ],
   "source": [
    "auc_scores=[]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "best_score=-1\n",
    "best_model=None\n",
    "threshold=0.9638\n",
    "print('Training model...')\n",
    "if need_validataion:\n",
    "    for i in range(nb_epoch):\n",
    "    #early_stopping=EarlyStopping(monitor='val_loss', patience=0, verbose=1)\n",
    "    #model.fit(X_train, y_train, nb_epoch=nb_epoch,batch_size=256,validation_split=0.01,callbacks=[early_stopping])\n",
    "        print (\"best_score is:\",best_score)\n",
    "        model.fit(X_train, y_train, nb_epoch=1,batch_size=256)\n",
    "        y_pre = model.predict_proba(X_valid)\n",
    "        scores = roc_auc_score(y_valid,y_pre)\n",
    "        auc_scores.append(scores)\n",
    "        print (i,scores)\n",
    "        if scores>best_score:\n",
    "            best_score=scores\n",
    "            best_model=model\n",
    "            if best_score>threshold:\n",
    "                y_pre = model.predict_proba(X_test)[:,1]\n",
    "                save2model(submission,'keras_nn_test_'+str(best_score)+\".csv\", y_pre)\n",
    "    plt.plot(auc_scores)\n",
    "    plt.show()\n",
    "else:\n",
    "    model.fit(X_train, y_train, nb_epoch=nb_epoch, batch_size=256)\n",
    "\n",
    "if need_validataion:\n",
    "    model=best_model\n",
    "#print('Generating submission...')\n",
    "y_pre = model.predict_proba(X_test)[:,1]\n",
    "#print roc_auc_score(y_test,y_pre)\n",
    "save2model(submission, 'keras_nn_test.csv',y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
